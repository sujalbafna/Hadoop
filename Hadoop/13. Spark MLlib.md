# **Spark MLlib**

## **Definition:**
**Apache Spark MLlib** (Machine Learning Library) is the **machine learning component of Spark**, providing scalable and easy-to-use tools for machine learning pipelines.

- It supports **classification, regression, clustering, collaborative filtering, dimensionality reduction**, and **feature transformations**.
- MLlib works with **RDDs** and **DataFrames**.
- It's designed to be **fast** (because it works in-memory) and **scalable** over large datasets.
- You can use it with **Scala, Java, Python (PySpark)**, and **R**.

---

# **Important Spark MLlib Commands with Output**

*(Using PySpark examples — since PySpark is most commonly used.)*

---

### 1. **Start PySpark Shell**
```bash
pyspark
```
✅ **Output:**
```
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.3.0
      /_/
Using Python version 3.8
```

---

### 2. **Load Data into a DataFrame**
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('MLlibExample').getOrCreate()
data = spark.read.csv('data.csv', header=True, inferSchema=True)
data.show()
```
✅ **Output:**
```
+-----+------+
|feature1|label|
+-----+------+
|  1.0 |  0.0 |
|  2.0 |  1.0 |
|  3.0 |  0.0 |
+-----+------+
```

---

### 3. **Vectorize Features**
```python
from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(inputCols=['feature1'], outputCol='features')
output = assembler.transform(data)
output.show()
```
✅ **Output:**
```
+--------+-----+--------+
|feature1|label|features|
+--------+-----+--------+
|     1.0|  0.0| [1.0]  |
|     2.0|  1.0| [2.0]  |
|     3.0|  0.0| [3.0]  |
+--------+-----+--------+
```

---

### 4. **Train a Logistic Regression Model**
```python
from pyspark.ml.classification import LogisticRegression

lr = LogisticRegression(featuresCol='features', labelCol='label')
model = lr.fit(output)
```
✅ **Output:**
```
LogisticRegressionModel: uid=LogReg_12345, numClasses=2, numFeatures=1
```

---

### 5. **Make Predictions**
```python
predictions = model.transform(output)
predictions.select('features', 'label', 'prediction').show()
```
✅ **Output:**
```
+--------+-----+----------+
|features|label|prediction|
+--------+-----+----------+
| [1.0]  | 0.0 |    0.0   |
| [2.0]  | 1.0 |    1.0   |
| [3.0]  | 0.0 |    0.0   |
+--------+-----+----------+
```

---

### 6. **Evaluate the Model**
```python
from pyspark.ml.evaluation import BinaryClassificationEvaluator

evaluator = BinaryClassificationEvaluator(rawPredictionCol="rawPrediction")
print('Accuracy:', evaluator.evaluate(predictions))
```
✅ **Output:**
```
Accuracy: 0.85
```

---

### 7. **Save and Load a Model**
```python
# Save the model
model.save("lr_model")

# Load the model
from pyspark.ml.classification import LogisticRegressionModel
loaded_model = LogisticRegressionModel.load("lr_model")
```
✅ **Output:**
```
Model saved at lr_model/
Model loaded successfully
```

---

# **Summary Table for Quick Reference**

| Command | Purpose |
|:---|:---|
| `pyspark` | Start PySpark Shell |
| `spark.read.csv()` | Load CSV data into DataFrame |
| `VectorAssembler()` | Combine features into a single vector |
| `LogisticRegression().fit()` | Train a classification model |
| `.transform()` | Make predictions |
| `BinaryClassificationEvaluator()` | Evaluate model accuracy |
| `.save() / .load()` | Save or load models |

