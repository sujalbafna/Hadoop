**Sqoop (Data Ingestion in Hadoop Ecosystem)**

**Definition:**
Apache Sqoop (SQL-to-Hadoop) is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured data stores such as relational databases (like MySQL, Oracle). Sqoop automates most of the process, relying on the database to describe the schema and using MapReduce to import and export the data.

---

**Basic Sqoop Commands with Example Outputs:**

1. **Check Sqoop Version**
```bash
sqoop version
```
**Output:**
```
Sqoop 1.4.7
...
```

2. **List Databases**
```bash
sqoop list-databases --connect jdbc:mysql://localhost/company_db --username root --password root
```
**Output:**
```
information_schema
company_db
test
```

3. **List Tables**
```bash
sqoop list-tables --connect jdbc:mysql://localhost/company_db --username root --password root
```
**Output:**
```
employees
departments
salaries
```

4. **Import a Table into HDFS**
```bash
sqoop import --connect jdbc:mysql://localhost/company_db --username root --password root --table employees --m 1 --target-dir /user/hadoop/employees
```
**Output:**
```
...
MapReduce Job Complete: 100% Complete
...
INFO mapreduce.ImportJobBase: Transferred 2.5 MB in 23.45 seconds
INFO mapreduce.ImportJobBase: Retrieved 5000 records.
```

5. **Import a Table with Query**
```bash
sqoop import --connect jdbc:mysql://localhost/company_db --username root --password root --query "SELECT id, name FROM employees WHERE salary > 50000 AND $CONDITIONS" --split-by id --target-dir /user/hadoop/high_salary_employees
```
**Output:**
```
Job successful.
```

6. **Import Specific Columns**
```bash
sqoop import --connect jdbc:mysql://localhost/company_db --username root --password root --table employees --columns "id,name" --target-dir /user/hadoop/selected_employees --m 1
```
**Output:**
```
INFO mapreduce.ImportJobBase: Retrieved 5000 records.
```

7. **Export Data from HDFS to MySQL**
```bash
sqoop export --connect jdbc:mysql://localhost/company_db --username root --password root --table new_employees --export-dir /user/hadoop/new_employees
```
**Output:**
```
Exported 5000 records.
```

8. **Incremental Import (Append Mode)**
```bash
sqoop import --connect jdbc:mysql://localhost/company_db --username root --password root --table employees --incremental append --check-column id --last-value 1000 --target-dir /user/hadoop/employees_incremental
```
**Output:**
```
INFO mapreduce.ImportJobBase: Retrieved 200 records.
```

9. **Create a Java Class for Table (Codegen)**
```bash
sqoop codegen --connect jdbc:mysql://localhost/company_db --username root --password root --table employees
```
**Output:**
```
Generating Java class for table employees
```

10. **Merge Two Datasets in HDFS**
```bash
sqoop merge --new-data /user/hadoop/new --onto /user/hadoop/old --target-dir /user/hadoop/merged --class-name employees --merge-key id
```
**Output:**
```
Merging completed successfully.
```

---

**Summary:**
Apache Sqoop efficiently bridges the gap between traditional relational databases and Hadoop by enabling bulk import/export operations. It simplifies data movement tasks crucial for big data analytics workflows.

