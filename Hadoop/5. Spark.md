**Spark (Data Processing in Hadoop Ecosystem)**

**Definition:**
Apache Spark is an open-source distributed processing system used for big data workloads. It utilizes in-memory caching, and optimized query execution for fast analytic queries against data of any size. Spark supports multiple programming languages (Java, Scala, Python, R) and provides high-level APIs for distributed data processing as well as libraries for SQL, streaming, machine learning (MLlib), and graph processing (GraphX).

---

**Basic Spark Commands with Example Outputs:**

1. **Start Spark Shell (Scala)**
```bash
spark-shell
```
**Output:**
```
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\
      /_/

Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_181)
Type in expressions to have them evaluated.
```

2. **Start PySpark Shell (Python)**
```bash
pyspark
```
**Output:**
```
Python 3.8.10 (default, May  3 2021, 08:55:58)
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\
      /_/

Using Python version 3.8.10
SparkSession available as 'spark'.
```

3. **Run a Spark Application**
```bash
spark-submit --class org.apache.spark.examples.SparkPi --master yarn /path/to/examples.jar 10
```
**Output:**
```
Pi is roughly 3.141592653589793
```

4. **Load a File into RDD**
```scala
val textFile = spark.read.textFile("hdfs://namenode:8020/input/data.txt")
```
**Output:**
```
textFile: org.apache.spark.sql.Dataset[String] = [value: string]
```

5. **Perform a Word Count (Scala)**
```scala
val counts = textFile.flatMap(line => line.split(" ")).groupByKey(identity).count()
counts.show()
```
**Output:**
```
+------+-----+
| word |count|
+------+-----+
| Hadoop|  5 |
| Spark |  3 |
| Data  |  7 |
+------+-----+
```

6. **Save Results Back to HDFS**
```scala
counts.write.format("csv").save("hdfs://namenode:8020/output/wordcount")
```
**Output:**
```
(no console output, saves file in HDFS)
```

7. **List Running Spark Applications on YARN**
```bash
yarn application -list
```
**Output:**
```
Total number of applications (application-types: [SPARK], states: [RUNNING]): 1
```

8. **Kill a Spark Application**
```bash
yarn application -kill <application_id>
```
**Output:**
```
Killing application application_168243_0005
```

9. **Check Spark History Server UI**
```
URL: http://<ResourceManager_IP>:18080/
```
**Output:**
- Opens the Spark History Server showing completed and running Spark jobs.

10. **Submit a PySpark Script**
```bash
spark-submit --master yarn pyspark_script.py
```
**Output:**
```
Script execution completed successfully.
```

---

**Summary:**
Apache Spark has revolutionized data processing by offering a faster and more flexible engine compared to traditional MapReduce. It supports various operations, in-memory computation, and a variety of tools for advanced analytics like SQL queries, streaming data, machine learning, and graph computations.

