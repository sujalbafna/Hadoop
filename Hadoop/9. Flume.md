**Flume (Data Ingestion in Hadoop Ecosystem)**

**Definition:**
Apache Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of streaming data into the Hadoop Distributed File System (HDFS). It is designed to handle large volumes of event-based data, such as log files generated by web servers, and supports a variety of sources, channels, and sinks.

---

**Basic Flume Commands with Example Outputs:**

1. **Check Flume Version**
```bash
flume-ng version
```
**Output:**
```
Flume 1.9.0
...
```

2. **Start Flume Agent (Simple Console Example)**
```bash
flume-ng agent --conf ./conf/ -f conf/flume-conf.properties.template -n agent1 -Dflume.root.logger=INFO,console
```
**Output:**
```
INFO node.PollableSourceRunner: Source runner starting
INFO lifecycle.LifecycleSupervisor: Starting SinkRunner: SinkRunner
...
```

3. **Sample Flume Configuration (conf/flume-conf.properties.template)**
```properties
agent1.sources = source1
agent1.sinks = sink1
agent1.channels = channel1

agent1.sources.source1.type = netcat
agent1.sources.source1.bind = localhost
agent1.sources.source1.port = 44444

agent1.sinks.sink1.type = logger

agent1.channels.channel1.type = memory
agent1.sources.source1.channels = channel1
agent1.sinks.sink1.channel = channel1
```

4. **Send Data to Flume (Using Netcat)**
```bash
echo "Hello Flume" | nc localhost 44444
```
**Output in Flume Console:**
```
INFO sink.LoggerSink: Event: { headers:{} body: 48 65 6C 6C 6F 20 46 6C 75 6D 65 Hello Flume }
```

5. **Flume Agent with HDFS Sink**
```bash
flume-ng agent --conf ./conf/ -f conf/flume-hdfs.conf -n agent1 -Dflume.root.logger=INFO,console
```

Sample HDFS Configuration (conf/flume-hdfs.conf):
```properties
agent1.sources = source1
agent1.sinks = sink1
agent1.channels = channel1

agent1.sources.source1.type = netcat
agent1.sources.source1.bind = localhost
agent1.sources.source1.port = 44445

agent1.sinks.sink1.type = hdfs
agent1.sinks.sink1.hdfs.path = hdfs://localhost:9000/user/hadoop/flume_output
agent1.sinks.sink1.hdfs.fileType = DataStream

agent1.channels.channel1.type = memory
agent1.sources.source1.channels = channel1
agent1.sinks.sink1.channel = channel1
```

6. **Check HDFS Output Directory**
```bash
hdfs dfs -ls /user/hadoop/flume_output
```
**Output:**
```
Found 1 items
-rw-r--r--   1 hadoop supergroup       56 2025-04-27 12:00 /user/hadoop/flume_output/FlumeData.1618912345678
```

7. **View Flume Output File Content**
```bash
hdfs dfs -cat /user/hadoop/flume_output/FlumeData.1618912345678
```
**Output:**
```
Hello Flume
Another Message
```

8. **Stop Flume Agent**
```bash
CTRL+C
```
**Output:**
```
INFO node.Application: Shutting down application Agent
...
```

---

**Summary:**
Apache Flume is an effective and scalable tool for streaming data ingestion into Hadoop environments. It allows real-time ingestion and reliable delivery, especially useful for collecting event logs, application data, and network traffic streams.

